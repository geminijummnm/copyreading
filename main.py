import json
import re
import os
from collections import defaultdict
from urllib.parse import urlparse

def get_base_domain(url):
    """ä»URLä¸­æå–ä¸»åŸŸåï¼Œç”¨äºæ²¡æœ‰æ˜ç¡®åç§°çš„æƒ…å†µ"""
    try:
        parsed_url = urlparse(url)
        # æå–åŸŸåéƒ¨åˆ†
        domain = parsed_url.netloc
        if domain:
            return domain
        else:
            # å¤„ç†æ²¡æœ‰ scheme çš„ URL
            parts = url.strip('/').split('/')
            if len(parts) >= 2:
                return parts[1]
            return parts[0]
    except Exception as e:
        print(f"æå–åŸŸåæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return ""

def get_next_id():
    """ä»pm.jsonæ–‡ä»¶ä¸­è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„åˆ†ç»„ID"""
    pm_file_path = 'pm.json'
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶ï¼š{pm_file_path}")
    if not os.path.exists(pm_file_path):
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ° pm.json æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        return None
    
    try:
        with open(pm_file_path, 'r', encoding='utf-8') as f:
            pm_data = json.load(f)
            if 'groups' in pm_data and isinstance(pm_data['groups'], list) and pm_data['groups']:
                numeric_groups = [item for item in pm_data['groups'] if isinstance(item, int)]
                if numeric_groups:
                    return max(numeric_groups) + 1
                else:
                    print("è­¦å‘Šï¼špm.json çš„ 'groups' åˆ—è¡¨ä¸ºç©ºæˆ–ä¸åŒ…å«æœ‰æ•ˆæ•°å­—ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
                    return 0
            else:
                print("è­¦å‘Šï¼špm.json æ–‡ä»¶ä¸­ 'groups' é”®ä¸å­˜åœ¨æˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
                return 0
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"è¯»å–æˆ–è§£æ pm.json æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return None

def process_txt_files():
    """
    å¤„ç†å½“å‰ç›®å½•ä¸‹æ‰€æœ‰txtæ–‡ä»¶ï¼Œæå–åˆ†ç»„ä¿¡æ¯ï¼Œç”Ÿæˆ.jsonæ–‡ä»¶å¹¶æ›´æ–°pm.json
    """
    next_id = get_next_id()
    if next_id is None:
        return

    txt_files = [f for f in os.listdir('.') if f.endswith('.txt')]
    if not txt_files:
        print("å½“å‰ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶ã€‚")
        return
        
    print(f"å·²æ‰¾åˆ° {len(txt_files)} ä¸ª .txt æ–‡ä»¶ã€‚")
    all_new_entries = []
    
    # ç¼–è¯‘å¤šç§æ­£åˆ™è¡¨è¾¾å¼ä»¥åº”å¯¹ä¸åŒæ ¼å¼
    # Pattern 1: å¸¦è¡¨æƒ…ç¬¦å·çš„æ ¼å¼
    pattern1 = re.compile(r'ğŸ“‹\s*æœºåœºåç§°:\s*(.+)\nğŸ”—\s*è®¢é˜…é“¾æ¥:\s*(.+)', re.MULTILINE)
    # Pattern 2: ä¸å¸¦è¡¨æƒ…ç¬¦å·çš„æ ¼å¼
    pattern2 = re.compile(r'æœºåœºåç§°:\s*(.+)\nè®¢é˜…é“¾æ¥:\s*(.+)', re.MULTILINE)
    # Pattern 3: ä»…URL
    pattern3 = re.compile(r'https?://[^\s]+', re.MULTILINE)

    for file_name in txt_files:
        print(f"\n--- æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š{file_name} ---")
        try:
            with open(file_name, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            continue

        entries_found = False
        
        # å°è¯•ä½¿ç”¨ç¬¬ä¸€ç§æ¨¡å¼
        matches = pattern1.findall(content)
        if matches:
            for name, url in matches:
                all_new_entries.append({'name': name.strip(), 'url': url.strip()})
            entries_found = True
        
        # å¦‚æœç¬¬ä¸€ç§æ¨¡å¼å¤±è´¥ï¼Œå°è¯•ç¬¬äºŒç§æ¨¡å¼
        if not entries_found:
            matches = pattern2.findall(content)
            if matches:
                for name, url in matches:
                    all_new_entries.append({'name': name.strip(), 'url': url.strip()})
                entries_found = True

        # å¦‚æœå‰ä¸¤ç§æ¨¡å¼éƒ½å¤±è´¥ï¼Œå°è¯•ä»…æå–URL
        if not entries_found:
            print(f"æ–‡ä»¶ä¸­æœªæ‰¾åˆ°â€œæœºåœºåç§°â€å’Œâ€œè®¢é˜…é“¾æ¥â€æ ‡ç­¾ï¼Œå°†å°è¯•æå–æ‰€æœ‰URLã€‚")
            urls_only = pattern3.findall(content)
            if urls_only:
                for url in urls_only:
                    name = get_base_domain(url.strip())
                    if name:
                        all_new_entries.append({'name': name, 'url': url.strip()})
        
        if not matches and not urls_only:
            print("æœªåœ¨æ­¤æ–‡ä»¶ä¸­æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„åˆ†ç»„ä¿¡æ¯ã€‚")


    if not all_new_entries:
        print("\næ‰€æœ‰æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½•æ–°çš„åˆ†ç»„ä¿¡æ¯ã€‚")
        return

    name_counts = defaultdict(int)
    processed_entries = set()
    final_entries = []

    for entry in all_new_entries:
        name = entry['name']
        url = entry['url']
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå®Œå…¨é‡å¤çš„æ¡ç›®
        if (name, url) in processed_entries:
            print(f"è·³è¿‡å®Œå…¨é‡å¤æ¡ç›®ï¼šåç§°='{name}', è®¢é˜…é“¾æ¥='{url}'")
            continue
        processed_entries.add((name, url))
        
        base_name = name
        current_name = name
        
        # æ£€æŸ¥åç§°æ˜¯å¦é‡å¤ï¼Œå¹¶è¿›è¡Œç¼–å·
        if base_name in name_counts:
             current_name = f"{base_name} ({name_counts[base_name]})"
        
        name_counts[base_name] += 1
        final_entries.append({'name': current_name, 'url': url})

    new_ids = []
    print(f"\nå³å°†åˆ›å»º {len(final_entries)} ä¸ªæ–°çš„åˆ†ç»„æ–‡ä»¶ã€‚")
    
    for entry in final_entries:
        file_name = f"{next_id}.json"
        
        config_data = {
            "archive": False,
            "front_proxy_id": -1,
            "id": next_id,
            "info": "",
            "landing_proxy_id": -1,
            "lastup": 0,
            "manually_column_width": False,
            "name": entry['name'],
            "skip_auto_update": False,
            "url": entry['url']
        }

        try:
            with open(file_name, 'w', encoding='utf-8') as outfile:
                json.dump(config_data, outfile, ensure_ascii=False, indent=4)
            print(f"å·²åˆ›å»ºæ–‡ä»¶ï¼š{file_name}ï¼Œåç§°ï¼š{entry['name']}")
            new_ids.append(next_id)
        except Exception as e:
            print(f"å†™å…¥æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            
        next_id += 1

    # æ›´æ–° pm.json æ–‡ä»¶
    pm_file_path = 'pm.json'
    if new_ids:
        try:
            with open(pm_file_path, 'r+', encoding='utf-8') as pm_file:
                pm_data = json.load(pm_file)
                pm_data['groups'].extend(new_ids)
                pm_file.seek(0)
                json.dump(pm_data, pm_file, ensure_ascii=False, indent=4)
                pm_file.truncate()
            print(f"\npm.json æ–‡ä»¶å·²æ›´æ–°ï¼Œæ·»åŠ äº† {len(new_ids)} ä¸ªæ–°çš„åˆ†ç»„ IDã€‚")
            print("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
        except Exception as e:
            print(f"\næ›´æ–° pm.json æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    else:
        print("\næ²¡æœ‰æ–°çš„åˆ†ç»„éœ€è¦æ·»åŠ ï¼Œpm.json æœªæ›´æ–°ã€‚")
        
if __name__ == '__main__':
    try:
        process_txt_files()
    except Exception as e:
        print(f"\nè„šæœ¬è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°æœªæ•è·çš„ä¸¥é‡é”™è¯¯ï¼š{e}")
