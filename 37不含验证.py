import json
import re
import os
from collections import defaultdict
from urllib.parse import urlparse
import sys
import yaml
import uuid

# æ£€æŸ¥ yaml åº“æ˜¯å¦å·²å®‰è£…
try:
    import yaml
except ImportError:
    print("é”™è¯¯: ç¼ºå°‘å¿…è¦çš„åº“ã€‚è¯·å…ˆä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š")
    print("pip install pyyaml")
    sys.exit()

def get_base_domain(url):
    """ä»URLä¸­æå–ä¸»åŸŸåï¼Œç”¨äºæ²¡æœ‰æ˜ç¡®åç§°çš„æƒ…å†µ"""
    try:
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        if domain:
            return domain
        else:
            parts = url.strip('/').split('/')
            if len(parts) >= 2:
                return parts[1]
            return parts[0]
    except Exception as e:
        return ""

def get_next_id():
    """ä»pm.jsonæ–‡ä»¶ä¸­è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„åˆ†ç»„ID"""
    pm_file_path = 'pm.json'
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶ï¼š{pm_file_path}")
    if not os.path.exists(pm_file_path):
        print("è­¦å‘Šï¼šæœªæ‰¾åˆ° pm.json æ–‡ä»¶ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
        return 0
    
    try:
        with open(pm_file_path, 'r', encoding='utf-8') as f:
            pm_data = json.load(f)
            if 'groups' in pm_data and isinstance(pm_data['groups'], list) and pm_data['groups']:
                numeric_groups = [item for item in pm_data['groups'] if isinstance(item, int)]
                if numeric_groups:
                    return max(numeric_groups) + 1
                else:
                    print("è­¦å‘Šï¼špm.json çš„ 'groups' åˆ—è¡¨ä¸ºç©ºæˆ–ä¸åŒ…å«æœ‰æ•ˆæ•°å­—ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
                    return 0
            else:
                print("è­¦å‘Šï¼špm.json æ–‡ä»¶ä¸­ 'groups' é”®ä¸å­˜åœ¨æˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
                return 0
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"è¯»å–æˆ–è§£æ pm.json æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
        return 0

def extract_subscriptions_from_files():
    """ä»å¤šç§æ–‡ä»¶ä¸­æå–è®¢é˜…é“¾æ¥ï¼Œå¹¶è¿›è¡Œå»é‡"""
    all_new_entries = []
    
    txt_files = [f for f in os.listdir('.') if f.endswith('.txt')]
    json_files = [f for f in os.listdir('.') if f.endswith('.json') and f != 'pm.json']
    yaml_files = [f for f in os.listdir('.') if f.endswith('.yaml') or f.endswith('.yml')]

    # ä»TXTæ–‡ä»¶ä¸­æå–
    txt_count = 0
    pattern1 = re.compile(r'ğŸ“‹\s*æœºåœºåç§°:\s*(.+)\nğŸ”—\s*è®¢é˜…é“¾æ¥:\s*(.+)', re.MULTILINE)
    pattern2 = re.compile(r'æœºåœºåç§°:\s*(.+)\nè®¢é˜…é“¾æ¥:\s*(.+)', re.MULTILINE)
    pattern3 = re.compile(r'https?://[^\s]+', re.MULTILINE)
    for file_name in txt_files:
        try:
            with open(file_name, 'r', encoding='utf-8') as f:
                content = f.read()
                matches = pattern1.findall(content)
                if not matches:
                    matches = pattern2.findall(content)
                if matches:
                    for name, url in matches:
                        all_new_entries.append({'name': name.strip(), 'url': url.strip()})
                    txt_count += len(matches)
                else:
                    urls_only = pattern3.findall(content)
                    if urls_only:
                        for url in urls_only:
                            url = url.strip()
                            if url:
                                name = get_base_domain(url)
                                if name:
                                    all_new_entries.append({'name': name, 'url': url})
                        txt_count += len(urls_only)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    print(f"è¯†åˆ«åˆ° TXT æ–‡ä»¶ {len(txt_files)} ä¸ªï¼Œè·å–è®¢é˜… {txt_count} ä¸ªã€‚")

    # ä»JSONæ–‡ä»¶ä¸­æå–
    json_count = 0
    for file_name in json_files:
        try:
            with open(file_name, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, dict) and 'name' in data and 'url' in data:
                    all_new_entries.append({'name': data['name'].strip(), 'url': data['url'].strip()})
                    json_count += 1
        except Exception as e:
            pass
    print(f"è¯†åˆ«åˆ° JSON æ–‡ä»¶ {len(json_files)} ä¸ªï¼Œè·å–è®¢é˜… {json_count} ä¸ªã€‚")

    # ä»YAMLæ–‡ä»¶ä¸­æå–
    yaml_count = 0
    for file_name in yaml_files:
        try:
            with open(file_name, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                if isinstance(data, dict) and 'proxies' in data:
                    for proxy in data['proxies']:
                        if isinstance(proxy, dict) and 'name' in proxy and 'url' in proxy:
                            all_new_entries.append({'name': proxy['name'].strip(), 'url': proxy['url'].strip()})
                            yaml_count += 1
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and 'url' in item:
                            name = item.get('name', get_base_domain(item['url']))
                            all_new_entries.append({'name': name.strip(), 'url': item['url'].strip()})
                            yaml_count += 1
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    print(f"è¯†åˆ«åˆ° YAML æ–‡ä»¶ {len(yaml_files)} ä¸ªï¼Œè·å–è®¢é˜… {yaml_count} ä¸ªã€‚")
    
    return all_new_entries

def deduplicate_with_existing(entries, choice):
    """æ ¹æ®ç”¨æˆ·é€‰æ‹©ï¼Œè¿›è¡Œç¬¬äºŒæ­¥å¤–éƒ¨å»é‡"""
    existing_urls = set()
    deduplicated_entries = []
    
    if choice == 'nekobox':
        json_files = [f for f in os.listdir('.') if f.endswith('.json') and f != 'pm.json']
        for file_name in json_files:
            try:
                with open(file_name, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict) and 'url' in data:
                        existing_urls.add(data['url'].strip())
            except Exception as e:
                print(f"è¯»å–ç°æœ‰ JSON æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    
    elif choice == 'singbox':
        yaml_file_path = 'subscribes.yaml'
        if os.path.exists(yaml_file_path):
            try:
                with open(yaml_file_path, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)
                    proxies_list = []
                    if isinstance(data, dict) and 'proxies' in data:
                        proxies_list = data['proxies']
                    elif isinstance(data, list):
                        proxies_list = data

                    for proxy in proxies_list:
                        if isinstance(proxy, dict) and 'url' in proxy:
                            existing_urls.add(proxy['url'].strip())
            except Exception as e:
                print(f"è¯»å–ç°æœ‰ YAML æ–‡ä»¶ {yaml_file_path} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    
    processed_new_entries = set()
    for entry in entries:
        if (entry['name'], entry['url']) in processed_new_entries:
            continue
        processed_new_entries.add((entry['name'], entry['url']))
        
        if entry['url'] not in existing_urls:
            deduplicated_entries.append(entry)
    
    return deduplicated_entries
    
def write_singbox_yaml(final_entries):
    """å°†æˆåŠŸçš„è®¢é˜…å†™å…¥ sing-box é…ç½®æ–‡ä»¶"""
    yaml_file_path = 'subscribes.yaml'
    
    existing_data = None
    if os.path.exists(yaml_file_path):
        try:
            with open(yaml_file_path, 'r', encoding='utf-8') as f:
                existing_data = yaml.safe_load(f)
        except Exception as e:
            print(f"è­¦å‘Šï¼šè¯»å–ç°æœ‰ {yaml_file_path} æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}ã€‚å°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
            existing_data = None

    # å‡†å¤‡è¦æ·»åŠ çš„æ–°è®¢é˜…é¡¹ï¼Œå¹¶å¡«å……æ‰€æœ‰å¿…è¦å­—æ®µ
    new_proxies_to_add = []
    for entry in final_entries:
        new_entry = {
            'id': f"ID_{str(uuid.uuid4()).replace('-', '')[:10]}",
            'name': entry['name'],
            'upload': 0,
            'download': 0,
            'total': 0,
            'expire': 0,
            'updateTime': 0,
            'type': 'Http',
            'url': entry['url'],
            'website': '',
            'path': f"data/subscribes/{entry['name']}.json",
            'include': '',
            'exclude': '',
            'includeProtocol': '',
            'excludeProtocol': '',
            'proxyPrefix': '',
            'disabled': False,
            'inSecure': False,
            'requestMethod': 'GET',
            'header': {'request': {}, 'response': {}},
            'proxies': [],
            'script': "const onSubscribe = async (proxies, subscription) => {\\n  return { proxies,\\\r\n    \\ subscription }\\n}\\n"
        }
        new_proxies_to_add.append(new_entry)
    
    # æ ¹æ®ç°æœ‰æ–‡ä»¶çš„æ ¼å¼å†³å®šå¦‚ä½•åˆå¹¶
    final_data = {}
    if existing_data is None:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„æ ‡å‡†åˆ—è¡¨æ ¼å¼
        final_data = new_proxies_to_add
    elif isinstance(existing_data, dict) and 'proxies' in existing_data:
        # å¦‚æœæ˜¯ {'proxies': [...] } æ ¼å¼ï¼Œåˆå¹¶åˆ°å…¶åˆ—è¡¨ä¸­
        existing_data['proxies'].extend(new_proxies_to_add)
        final_data = existing_data
    elif isinstance(existing_data, list):
        # å¦‚æœæ˜¯ç›´æ¥çš„åˆ—è¡¨æ ¼å¼ï¼Œç›´æ¥è¿½åŠ åˆ°åˆ—è¡¨ä¸­
        final_data = existing_data + new_proxies_to_add
    else:
        # æ— æ³•è¯†åˆ«çš„æ ¼å¼ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„æ ‡å‡†åˆ—è¡¨æ ¼å¼
        print(f"è­¦å‘Šï¼š{yaml_file_path} æ–‡ä»¶æ ¼å¼æ— æ³•è¯†åˆ«ï¼Œå°†åˆ›å»ºæ–°çš„æ ‡å‡†åˆ—è¡¨æ ¼å¼æ–‡ä»¶ã€‚")
        final_data = new_proxies_to_add

    try:
        with open(yaml_file_path, 'w', encoding='utf-8') as f:
            yaml.dump(final_data, f, allow_unicode=True, indent=2, sort_keys=False)
        print(f"Singbox é…ç½®æ–‡ä»¶ '{yaml_file_path}' å·²æˆåŠŸæ›´æ–°ã€‚")
    except Exception as e:
        print(f"å†™å…¥ Singbox é…ç½®æ–‡ä»¶ '{yaml_file_path}' æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def write_nekobox_json(final_entries):
    """å°†æˆåŠŸçš„è®¢é˜…å†™å…¥ nekobox é…ç½®æ–‡ä»¶"""
    next_id = get_next_id()
    if next_id is None:
        return
        
    name_counts = defaultdict(int)
    new_ids = []
    for entry in final_entries:
        base_name = entry['name']
        current_name = base_name
        if name_counts[base_name] > 0:
             current_name = f"{base_name} ({name_counts[base_name]})"
        name_counts[base_name] += 1
        
        file_name = f"{next_id}.json"
        
        config_data = {
            "archive": False,
            "front_proxy_id": -1,
            "id": next_id,
            "info": "",
            "landing_proxy_id": -1,
            "lastup": 0,
            "manually_column_width": False,
            "name": current_name,
            "skip_auto_update": False,
            "url": entry['url']
        }

        try:
            with open(file_name, 'w', encoding='utf-8') as outfile:
                json.dump(config_data, outfile, ensure_ascii=False, indent=4)
            print(f"å·²åˆ›å»ºæ–‡ä»¶ï¼š{file_name}ï¼Œåç§°ï¼š{current_name}")
            new_ids.append(next_id)
        except Exception as e:
            print(f"å†™å…¥æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            
        next_id += 1

    pm_file_path = 'pm.json'
    if new_ids:
        try:
            with open(pm_file_path, 'r+', encoding='utf-8') as pm_file:
                pm_data = json.load(pm_file)
                # ç¡®ä¿ pm_data['groups'] æ˜¯ä¸€ä¸ªåˆ—è¡¨
                if not isinstance(pm_data.get('groups'), list):
                    pm_data['groups'] = []
                pm_data['groups'].extend(new_ids)
                pm_file.seek(0)
                json.dump(pm_data, pm_file, ensure_ascii=False, indent=4)
                pm_file.truncate()
            print(f"\npm.json æ–‡ä»¶å·²æ›´æ–°ï¼Œæ·»åŠ äº† {len(new_ids)} ä¸ªæ–°çš„åˆ†ç»„ IDã€‚")
        except Exception as e:
            print(f"\næ›´æ–° pm.json æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")


def main():
    while True:
        choice = input("è¯·é€‰æ‹©è¦ç”Ÿæˆçš„é…ç½®ç±»å‹ (nekobox/singbox): ").strip().lower()
        if choice in ['nekobox', 'singbox']:
            break
        print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ 'nekobox' æˆ– 'singbox'ã€‚")

    all_new_entries = extract_subscriptions_from_files()

    if not all_new_entries:
        print("\næ‰€æœ‰æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½•æ–°çš„åˆ†ç»„ä¿¡æ¯ã€‚")
        return
    
    final_entries = deduplicate_with_existing(all_new_entries, choice)

    if not final_entries:
        print("\næ‰€æœ‰æ–°è®¢é˜…å·²å­˜åœ¨äºç°æœ‰é…ç½®æ–‡ä»¶ä¸­ï¼Œæ— éœ€æ·»åŠ ã€‚")
        return

    print(f"\nå»é‡åå…±æ‰¾åˆ° {len(final_entries)} ä¸ªç‹¬ç«‹è®¢é˜…é“¾æ¥ã€‚")

    # å†™å…¥ suc.jpg
    suc_entries_formatted = [{'name': entry['name'], 'url': entry['url']} for entry in final_entries]
    with open('suc.jpg', 'w', encoding='utf-8') as f:
        json.dump(suc_entries_formatted, f, ensure_ascii=False, indent=4)
    print("å·²ç”Ÿæˆ suc.jpg æ–‡ä»¶ã€‚")

    # æ ¹æ®ç”¨æˆ·é€‰æ‹©ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶
    if choice == 'nekobox':
        write_nekobox_json(final_entries)
    elif choice == 'singbox':
        write_singbox_yaml(final_entries)

    print("\næ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nè„šæœ¬è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°æœªæ•è·çš„ä¸¥é‡é”™è¯¯ï¼š{e}")