import json
import re
import os
from collections import defaultdict
from urllib.parse import urlparse
import sys
import requests
from requests.exceptions import RequestException, ConnectionError, Timeout
from concurrent.futures import ThreadPoolExecutor
import datetime
import socks
import yaml

# æ£€æŸ¥ requests, PySocks å’Œ PyYAML åº“æ˜¯å¦å·²å®‰è£…
try:
    import requests
    import socks
    import yaml
except ImportError:
    print("é”™è¯¯: ç¼ºå°‘å¿…è¦çš„åº“ã€‚è¯·å…ˆä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š")
    print("pip install \"requests[socks]\" PySocks pyyaml")
    sys.exit()

def get_base_domain(url):
    """ä»URLä¸­æå–ä¸»åŸŸåï¼Œç”¨äºæ²¡æœ‰æ˜ç¡®åç§°çš„æƒ…å†µ"""
    try:
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        if domain:
            return domain
        else:
            parts = url.strip('/').split('/')
            if len(parts) >= 2:
                return parts[1]
            return parts[0]
    except Exception as e:
        return ""

def get_next_id():
    """ä»pm.jsonæ–‡ä»¶ä¸­è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„åˆ†ç»„ID"""
    pm_file_path = 'pm.json'
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶ï¼š{pm_file_path}")
    if not os.path.exists(pm_file_path):
        print("è­¦å‘Šï¼šæœªæ‰¾åˆ° pm.json æ–‡ä»¶ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
        return 0
    
    try:
        with open(pm_file_path, 'r', encoding='utf-8') as f:
            pm_data = json.load(f)
            if 'groups' in pm_data and isinstance(pm_data['groups'], list) and pm_data['groups']:
                numeric_groups = [item for item in pm_data['groups'] if isinstance(item, int)]
                if numeric_groups:
                    return max(numeric_groups) + 1
                else:
                    print("è­¦å‘Šï¼špm.json çš„ 'groups' åˆ—è¡¨ä¸ºç©ºæˆ–ä¸åŒ…å«æœ‰æ•ˆæ•°å­—ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
                    return 0
            else:
                print("è­¦å‘Šï¼špm.json æ–‡ä»¶ä¸­ 'groups' é”®ä¸å­˜åœ¨æˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
                return 0
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"è¯»å–æˆ–è§£æ pm.json æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}ï¼Œå°†ä» ID 0 å¼€å§‹ã€‚")
        return 0

def is_url_valid(entry, proxies_list):
    """éªŒè¯å•ä¸ª URLï¼Œå¹¶è¿”å›éªŒè¯ç»“æœå’Œå¤±è´¥åŸå› """
    url = entry['url']
    user_agents = {
        'chrome': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'clashmeta': 'Clash-Verge/1.3.1',
        'singbox': 'sing-box'
    }

    def try_request(current_proxies=None):
        for ua_name, ua_string in user_agents.items():
            headers = {'User-Agent': ua_string}
            current_time = datetime.datetime.now().strftime("%H:%M:%S")
            
            try:
                response = requests.head(url, timeout=15, allow_redirects=True, proxies=current_proxies, headers=headers)
                
                if 200 <= response.status_code < 400:
                    print(f"-> æ­£åœ¨{'ç›´æ¥' if not current_proxies else 'é€šè¿‡ä»£ç†'}éªŒè¯ URL: {url} ... UA:{ua_name} {current_time} æˆåŠŸï¼")
                    return True, "æˆåŠŸ"
                elif response.status_code in [403, 405, 429]:
                    print(f"-> æ­£åœ¨{'ç›´æ¥' if not current_proxies else 'é€šè¿‡ä»£ç†'}éªŒè¯ URL: {url} ... UA:{ua_name} {current_time} æˆåŠŸ (çŠ¶æ€ç : {response.status_code}, è§†ä¸ºæˆåŠŸï¼)")
                    return True, f"çŠ¶æ€ç : {response.status_code}"
                else:
                    print(f"-> æ­£åœ¨{'ç›´æ¥' if not current_proxies else 'é€šè¿‡ä»£ç†'}éªŒè¯ URL: {url} ... UA:{ua_name} {current_time} å¤±è´¥ (çŠ¶æ€ç : {response.status_code})ã€‚")
                    return False, f"çŠ¶æ€ç : {response.status_code}"
            except Timeout:
                print(f"-> æ­£åœ¨{'ç›´æ¥' if not current_proxies else 'é€šè¿‡ä»£ç†'}éªŒè¯ URL: {url} ... UA:{ua_name} {current_time} å¤±è´¥ (è¶…æ—¶)ã€‚")
                return False, "è¶…æ—¶"
            except ConnectionError as e:
                if '10054' in str(e):
                    print(f"-> æ­£åœ¨{'ç›´æ¥' if not current_proxies else 'é€šè¿‡ä»£ç†'}éªŒè¯ URL: {url} ... UA:{ua_name} {current_time} æˆåŠŸ (ç½‘ç»œè¿æ¥é”™è¯¯: 10054, è§†ä¸ºæˆåŠŸï¼)")
                    return True, "ç½‘ç»œè¿æ¥é”™è¯¯: 10054"
                elif '10053' in str(e):
                    print(f"-> æ­£åœ¨{'ç›´æ¥' if not current_proxies else 'é€šè¿‡ä»£ç†'}éªŒè¯ URL: {url} ... UA:{ua_name} {current_time} å¤±è´¥ (ç½‘ç»œè¿æ¥é”™è¯¯: 10053, å°†å°è¯•å…¶ä»–ä»£ç†)ã€‚")
                    continue
                else:
                    print(f"-> æ­£åœ¨{'ç›´æ¥' if not current_proxies else 'é€šè¿‡ä»£ç†'}éªŒè¯ URL: {url} ... UA:{ua_name} {current_time} å¤±è´¥ (ç½‘ç»œè¯·æ±‚é”™è¯¯: {e})ã€‚")
                    return False, f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}"
            except RequestException as e:
                print(f"-> æ­£åœ¨{'ç›´æ¥' if not current_proxies else 'é€šè¿‡ä»£ç†'}éªŒè¯ URL: {url} ... UA:{ua_name} {current_time} å¤±è´¥ (ç½‘ç»œè¯·æ±‚é”™è¯¯: {e})ã€‚")
                return False, f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}"
        return False, "æ‰€æœ‰UAå‡å¤±è´¥"

    is_success, reason = try_request()
    if is_success:
        return True, entry
    
    if proxies_list:
        print("æ­£åœ¨å°è¯•ä½¿ç”¨ä»£ç†...")
        for proxy_address in proxies_list:
            proxies = {
                "http": proxy_address,
                "https": proxy_address,
            }
            try:
                is_success, reason = try_request(proxies)
                if is_success:
                    return True, entry
            except (socks.SocksError, requests.exceptions.ProxyError) as e:
                print(f"è­¦å‘Š: ä»£ç† {proxy_address} è¿æ¥å¤±è´¥: {e}ã€‚å°†å°è¯•ä¸‹ä¸€ä¸ªä»£ç†ã€‚")
                reason = f"ä»£ç†è¿æ¥å¤±è´¥: {e}"
                continue
            except RequestException as e:
                print(f"è­¦å‘Š: ä»£ç† {proxy_address} éªŒè¯å¤±è´¥: {e}ã€‚å°†å°è¯•ä¸‹ä¸€ä¸ªä»£ç†ã€‚")
                reason = f"ä»£ç†éªŒè¯å¤±è´¥: {e}"
                continue
    
    print(f"æ‰€æœ‰å°è¯•å‡å¤±è´¥ã€‚é“¾æ¥: {url}")
    return False, {'name': entry['name'], 'url': url, 'failedReason': reason}

def extract_subscriptions_from_files():
    """ä»å¤šç§æ–‡ä»¶ä¸­æå–è®¢é˜…é“¾æ¥ï¼Œå¹¶è¿›è¡Œå»é‡"""
    all_new_entries = []
    
    txt_files = [f for f in os.listdir('.') if f.endswith('.txt')]
    json_files = [f for f in os.listdir('.') if f.endswith('.json') and f != 'pm.json']
    yaml_files = [f for f in os.listdir('.') if f.endswith('.yaml') or f.endswith('.yml')]

    # ä»TXTæ–‡ä»¶ä¸­æå–
    txt_count = 0
    pattern1 = re.compile(r'ğŸ“‹\s*æœºåœºåç§°:\s*(.+)\nğŸ”—\s*è®¢é˜…é“¾æ¥:\s*(.+)', re.MULTILINE)
    pattern2 = re.compile(r'æœºåœºåç§°:\s*(.+)\nè®¢é˜…é“¾æ¥:\s*(.+)', re.MULTILINE)
    pattern3 = re.compile(r'https?://[^\s]+', re.MULTILINE)
    for file_name in txt_files:
        try:
            with open(file_name, 'r', encoding='utf-8') as f:
                content = f.read()
                matches = pattern1.findall(content)
                if not matches:
                    matches = pattern2.findall(content)
                if matches:
                    for name, url in matches:
                        all_new_entries.append({'name': name.strip(), 'url': url.strip()})
                    txt_count += len(matches)
                else:
                    urls_only = pattern3.findall(content)
                    if urls_only:
                        for url in urls_only:
                            url = url.strip()
                            if url:
                                name = get_base_domain(url)
                                if name:
                                    all_new_entries.append({'name': name, 'url': url})
                        txt_count += len(urls_only)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    print(f"è¯†åˆ«åˆ° TXT æ–‡ä»¶ {len(txt_files)} ä¸ªï¼Œè·å–è®¢é˜… {txt_count} ä¸ªã€‚")

    # ä»JSONæ–‡ä»¶ä¸­æå–
    json_count = 0
    for file_name in json_files:
        try:
            with open(file_name, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, dict) and 'name' in data and 'url' in data:
                    all_new_entries.append({'name': data['name'].strip(), 'url': data['url'].strip()})
                    json_count += 1
        except Exception as e:
            pass
    print(f"è¯†åˆ«åˆ° JSON æ–‡ä»¶ {len(json_files)} ä¸ªï¼Œè·å–è®¢é˜… {json_count} ä¸ªã€‚")

    # ä»YAMLæ–‡ä»¶ä¸­æå–
    yaml_count = 0
    for file_name in yaml_files:
        try:
            with open(file_name, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                if isinstance(data, dict) and 'proxies' in data:
                    for proxy in data['proxies']:
                        if isinstance(proxy, dict) and 'name' in proxy and 'url' in proxy:
                            all_new_entries.append({'name': proxy['name'].strip(), 'url': proxy['url'].strip()})
                            yaml_count += 1
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and 'url' in item:
                            name = item.get('name', get_base_domain(item['url']))
                            all_new_entries.append({'name': name.strip(), 'url': item['url'].strip()})
                            yaml_count += 1
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    print(f"è¯†åˆ«åˆ° YAML æ–‡ä»¶ {len(yaml_files)} ä¸ªï¼Œè·å–è®¢é˜… {yaml_count} ä¸ªã€‚")
    
    return all_new_entries

def deduplicate_with_existing(entries, choice):
    """æ ¹æ®ç”¨æˆ·é€‰æ‹©ï¼Œè¿›è¡Œç¬¬äºŒæ­¥å¤–éƒ¨å»é‡"""
    existing_urls = set()
    deduplicated_entries = []
    
    if choice == 'nekobox':
        json_files = [f for f in os.listdir('.') if f.endswith('.json') and f != 'pm.json']
        for file_name in json_files:
            try:
                with open(file_name, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict) and 'url' in data:
                        existing_urls.add(data['url'].strip())
            except Exception as e:
                print(f"è¯»å–ç°æœ‰ JSON æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    
    elif choice == 'singbox':
        yaml_file_path = 'subscribes.yaml'
        if os.path.exists(yaml_file_path):
            try:
                with open(yaml_file_path, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)
                    if isinstance(data, dict) and 'proxies' in data:
                        proxies_list = data['proxies']
                    elif isinstance(data, list):
                        proxies_list = data
                    else:
                        proxies_list = []

                    for proxy in proxies_list:
                        if isinstance(proxy, dict) and 'url' in proxy:
                            existing_urls.add(proxy['url'].strip())
            except Exception as e:
                print(f"è¯»å–ç°æœ‰ YAML æ–‡ä»¶ {yaml_file_path} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    
    processed_new_entries = set()
    for entry in entries:
        if (entry['name'], entry['url']) in processed_new_entries:
            continue
        processed_new_entries.add((entry['name'], entry['url']))
        
        if entry['url'] not in existing_urls:
            deduplicated_entries.append(entry)
    
    return deduplicated_entries
    
def write_singbox_yaml(final_entries):
    """å°†æˆåŠŸçš„è®¢é˜…å†™å…¥ sing-box é…ç½®æ–‡ä»¶"""
    yaml_file_path = 'subscribes.yaml'
    
    existing_proxies = []
    if os.path.exists(yaml_file_path):
        try:
            with open(yaml_file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                if isinstance(data, dict) and 'proxies' in data:
                    existing_proxies = data['proxies']
                elif isinstance(data, list):
                    existing_proxies = data
        except Exception as e:
            print(f"è­¦å‘Šï¼šè¯»å–ç°æœ‰ {yaml_file_path} æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}ã€‚å°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
    
    new_proxies = []
    for entry in final_entries:
        new_proxies.append({'name': entry['name'], 'url': entry['url']})
    
    final_proxies = existing_proxies + new_proxies
    
    config_data = {'proxies': final_proxies}
    
    try:
        with open(yaml_file_path, 'w', encoding='utf-8') as f:
            yaml.dump(config_data, f, allow_unicode=True, indent=2, sort_keys=False)
        print(f"Singbox é…ç½®æ–‡ä»¶å·²æˆåŠŸæ›´æ–°ã€‚")
    except Exception as e:
        print(f"å†™å…¥ Singbox é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def write_nekobox_json(final_entries):
    """å°†æˆåŠŸçš„è®¢é˜…å†™å…¥ nekobox é…ç½®æ–‡ä»¶"""
    next_id = get_next_id()
    if next_id is None:
        return
        
    name_counts = defaultdict(int)
    new_ids = []
    for entry in final_entries:
        base_name = entry['name']
        current_name = base_name
        if name_counts[base_name] > 0:
             current_name = f"{base_name} ({name_counts[base_name]})"
        name_counts[base_name] += 1
        
        file_name = f"{next_id}.json"
        
        config_data = {
            "archive": False,
            "front_proxy_id": -1,
            "id": next_id,
            "info": "",
            "landing_proxy_id": -1,
            "lastup": 0,
            "manually_column_width": False,
            "name": current_name,
            "skip_auto_update": False,
            "url": entry['url']
        }

        try:
            with open(file_name, 'w', encoding='utf-8') as outfile:
                json.dump(config_data, outfile, ensure_ascii=False, indent=4)
            print(f"å·²åˆ›å»ºæ–‡ä»¶ï¼š{file_name}ï¼Œåç§°ï¼š{current_name}")
            new_ids.append(next_id)
        except Exception as e:
            print(f"å†™å…¥æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            
        next_id += 1

    pm_file_path = 'pm.json'
    if new_ids:
        try:
            with open(pm_file_path, 'r+', encoding='utf-8') as pm_file:
                pm_data = json.load(pm_file)
                pm_data['groups'].extend(new_ids)
                pm_file.seek(0)
                json.dump(pm_data, pm_file, ensure_ascii=False, indent=4)
                pm_file.truncate()
            print(f"\npm.json æ–‡ä»¶å·²æ›´æ–°ï¼Œæ·»åŠ äº† {len(new_ids)} ä¸ªæ–°çš„åˆ†ç»„ IDã€‚")
        except Exception as e:
            print(f"\næ›´æ–° pm.json æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")


def main():
    while True:
        choice = input("è¯·é€‰æ‹©è¦ç”Ÿæˆçš„é…ç½®ç±»å‹ (nekobox/singbox): ").strip().lower()
        if choice in ['nekobox', 'singbox']:
            break
        print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ 'nekobox' æˆ– 'singbox'ã€‚")

    all_new_entries = extract_subscriptions_from_files()

    if not all_new_entries:
        print("\næ‰€æœ‰æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½•æ–°çš„åˆ†ç»„ä¿¡æ¯ã€‚")
        return
    
    unique_entries = deduplicate_with_existing(all_new_entries, choice)

    if not unique_entries:
        print("\næ‰€æœ‰æ–°è®¢é˜…å·²å­˜åœ¨äºç°æœ‰é…ç½®æ–‡ä»¶ä¸­ï¼Œæ— éœ€æ·»åŠ ã€‚")
        return

    while True:
        try:
            num_threads = int(input("è¯·è¾“å…¥è¦ä½¿ç”¨çš„çº¿ç¨‹æ•° (å»ºè®®2-16ï¼Œæˆ–ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤8): ").strip() or "8")
            if num_threads > 0:
                break
            else:
                print("çº¿ç¨‹æ•°å¿…é¡»ä¸ºæ­£æ•´æ•°ã€‚")
        except ValueError:
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ä¸€ä¸ªæ•°å­—ã€‚")

    proxies_input = input("è¯·è¾“å…¥ä»£ç†åœ°å€ï¼Œå¤šä¸ªä»£ç†ç”¨è‹±æ–‡é€—å·åˆ†éš” (ä¾‹å¦‚: socks5://127.0.0.1:1080,http://192.168.1.1:8080ï¼Œæˆ–ç›´æ¥å›è½¦è·³è¿‡): ").strip()
    proxies_list = [p.strip() for p in proxies_input.split(',') if p.strip()]
    if proxies_list:
        print(f"å·²é…ç½® {len(proxies_list)} ä¸ªä»£ç†ï¼Œå°†ä»…åœ¨ç›´æ¥è®¿é—®å¤±è´¥æ—¶ä½¿ç”¨ã€‚")

    print(f"å»é‡åå…± {len(unique_entries)} æ¡ç‹¬ç«‹è®¢é˜…é“¾æ¥ï¼Œæ­£åœ¨ä½¿ç”¨ {num_threads} çº¿ç¨‹è¿›è¡ŒéªŒè¯...")

    final_entries = []
    failed_entries = []
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        results = executor.map(lambda e: is_url_valid(e, proxies_list), unique_entries)
        
        for is_success, result_data in results:
            if is_success:
                final_entries.append(result_data)
            else:
                failed_entries.append(result_data)

    print(f"\néªŒè¯å®Œæˆï¼Œå…±æ‰¾åˆ° {len(final_entries)} ä¸ªæœ‰æ•ˆè®¢é˜…é“¾æ¥ï¼Œ{len(failed_entries)} ä¸ªå¤±è´¥è®¢é˜…é“¾æ¥ã€‚")

    # å†™å…¥ suc.jpg
    suc_entries_formatted = [{'name': entry['name'], 'url': entry['url']} for entry in final_entries]
    with open('suc.jpg', 'w', encoding='utf-8') as f:
        json.dump(suc_entries_formatted, f, ensure_ascii=False, indent=4)
    print("å·²ç”Ÿæˆ suc.jpg æ–‡ä»¶ã€‚")

    # å†™å…¥ fa.jpg
    fa_entries_formatted = [{'name': entry['name'], 'url': entry['url']} for entry in failed_entries]
    with open('fa.jpg', 'w', encoding='utf-8') as f:
        json.dump(fa_entries_formatted, f, ensure_ascii=False, indent=4)
    print("å·²ç”Ÿæˆ fa.jpg æ–‡ä»¶ã€‚")

    # æ ¹æ®ç”¨æˆ·é€‰æ‹©ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶
    if choice == 'nekobox':
        write_nekobox_json(final_entries)
    elif choice == 'singbox':
        write_singbox_yaml(final_entries)

    print("\næ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nè„šæœ¬è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°æœªæ•è·çš„ä¸¥é‡é”™è¯¯ï¼š{e}")